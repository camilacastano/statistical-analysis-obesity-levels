{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olXOrqxNA-iI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os, math, json\n",
        "from scipy.stats import gaussian_kde\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "from sklearn.neighbors import KernelDensity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOouyKRZ71rj",
        "outputId": "df00aac0-f2c0-44ca-b363-3d473bb589f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHxidpMw7D8Z"
      },
      "outputs": [],
      "source": [
        "df ='/content/drive/MyDrive/math/ObesityDataSet_raw_and_data_sinthetic.csv'\n",
        "CSV_PATH = pd.read_csv(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIwo7Yk-ctXc"
      },
      "outputs": [],
      "source": [
        "obe = {\n",
        " 'Insufficient_Weight': 'Insufficient Weight',\n",
        " 'Normal_Weight': 'Normal Weight',\n",
        " 'Overweight_Level_I': 'Overweight I',\n",
        " 'Overweight_Level_II': 'Overweight II',\n",
        " 'Obesity_Type_I': 'Obesity I',\n",
        " 'Obesity_Type_II': 'Obesity II',\n",
        " 'Obesity_Type_III': 'Obesity III'\n",
        "}\n",
        "\n",
        "CSV_PATH['NObeyesdad_new'] = CSV_PATH['NObeyesdad'].map(obe).astype('category')\n",
        "\n",
        "continuous = ['Age', 'Height', 'Weight', 'FCVC', 'NCP']\n",
        "binary = ['FAVC','SMOKE']\n",
        "categorical = ['Gender','CAEC','MTRANS']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ggfknwsl5CH"
      },
      "outputs": [],
      "source": [
        "continuous_features = [\"Age\",\n",
        "                       \"Height\",\n",
        "                       \"Weight\",\n",
        "                       \"NCP\",\n",
        "                       \"FCVC\",\n",
        "                       \"CH2O\",\n",
        "                       \"FAF\",\n",
        "                       \"TUE\"]\n",
        "binary_features = [\"family_history_with_overweight\",\n",
        "                   \"FAVC\",\n",
        "                   \"SMOKE\",\n",
        "                   \"SCC\"]\n",
        "categorical_features = [\"Gender\",\n",
        "                        \"CAEC\",\n",
        "                        \"CALC\",\n",
        "                        \"MTRANS\",\n",
        "                        \"NObeyesdad_new\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xniFs4-9qvOY"
      },
      "source": [
        "# First task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pjNaui7cqu62"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "BOOT_N_PROPORTION = 2000\n",
        "BOOT_N_KDE = 500\n",
        "KDE_GRID_N = 500\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR,\"figs\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR,\"tables\"), exist_ok=True)\n",
        "\n",
        "def bootstrap_ci_values(data, statfunc=np.mean, n_boot=1000, ci=95):\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "    boots = []\n",
        "    for _ in range(n_boot):\n",
        "        s = np.random.choice(data, size=n, replace=True)\n",
        "        boots.append(statfunc(s))\n",
        "    lo = np.percentile(boots, (100-ci)/2)\n",
        "    hi = np.percentile(boots, 100-(100-ci)/2)\n",
        "    return lo, hi, boots\n",
        "\n",
        "def pmf_with_bootstrap(series, n_boot=1000):\n",
        "    vals = series.dropna()\n",
        "    counts = vals.value_counts().sort_index()\n",
        "    pmf = (counts / counts.sum()).astype(float)\n",
        "    rows = []\n",
        "    for cat in pmf.index:\n",
        "        binary = (vals == cat).astype(int).values\n",
        "        lo, hi, _ = bootstrap_ci_values(binary, statfunc=np.mean, n_boot=n_boot)\n",
        "        rows.append({\"category\": cat, \"pmf\": pmf[cat], \"ci_low\": lo, \"ci_high\": hi, \"count\": int(counts[cat])})\n",
        "    df_pmf = pd.DataFrame(rows).sort_values(\"pmf\", ascending=False)\n",
        "    return df_pmf\n",
        "\n",
        "def kde_pdf_cdf_with_boot(series, grid_n=500, boot_n=500):\n",
        "    data = series.dropna().values\n",
        "    if len(data) < 3:\n",
        "        raise ValueError(\"Not enough data for KDE\")\n",
        "    grid = np.linspace(data.min(), data.max(), grid_n)\n",
        "    kde = gaussian_kde(data)\n",
        "    pdf = kde(grid)\n",
        "    from numpy import trapz\n",
        "    pdf = pdf / trapz(pdf, grid)\n",
        "    cdf = np.cumsum(pdf)\n",
        "    cdf = cdf / cdf[-1]\n",
        "\n",
        "    pdf_boot = np.empty((boot_n, grid_n))\n",
        "    cdf_boot = np.empty((boot_n, grid_n))\n",
        "    for i in range(boot_n):\n",
        "        samp = np.random.choice(data, size=len(data), replace=True)\n",
        "        kv = gaussian_kde(samp)\n",
        "        p = kv(grid)\n",
        "        p = p / trapz(p, grid)\n",
        "        pdf_boot[i,:] = p\n",
        "        c = np.cumsum(p); c = c / c[-1]\n",
        "        cdf_boot[i,:] = c\n",
        "    pdf_low = np.percentile(pdf_boot, 2.5, axis=0)\n",
        "    pdf_high = np.percentile(pdf_boot, 97.5, axis=0)\n",
        "    cdf_low = np.percentile(cdf_boot, 2.5, axis=0)\n",
        "    cdf_high = np.percentile(cdf_boot, 97.5, axis=0)\n",
        "    return grid, pdf, pdf_low, pdf_high, cdf, cdf_low, cdf_high\n",
        "\n",
        "def bootstrap_mean_var_ci(series, n_boot=2000):\n",
        "    arr = np.array(series.dropna())\n",
        "    lo_m, hi_m, _ = bootstrap_ci_values(arr, statfunc=np.mean, n_boot=n_boot)\n",
        "    lo_v, hi_v, _ = bootstrap_ci_values(arr, statfunc=lambda x: x.var(ddof=1), n_boot=n_boot)\n",
        "    return {\"mean\": arr.mean(), \"mean_ci\": (lo_m, hi_m), \"var\": arr.var(ddof=1), \"var_ci\": (lo_v, hi_v)}\n",
        "\n",
        "def plot_pdf_cdf_combined(col, grid, pdf, pdf_low, pdf_high, cdf, cdf_low, cdf_high, out_dir):\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "    # PDF\n",
        "    axs[0].plot(grid, pdf, label=\"KDE PDF\")\n",
        "    axs[0].fill_between(grid, pdf_low, pdf_high, alpha=0.3, label=\"95% CI\")\n",
        "    axs[0].set_title(f\"PDF (KDE) — {col}\")\n",
        "    axs[0].set_xlabel(col)\n",
        "    axs[0].set_ylabel(\"Density\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    # CDF\n",
        "    axs[1].plot(grid, cdf, label=\"CDF\")\n",
        "    axs[1].fill_between(grid, cdf_low, cdf_high, alpha=0.3, label=\"95% CI\")\n",
        "    axs[1].set_title(f\"CDF (KDE) — {col}\")\n",
        "    axs[1].set_xlabel(col)\n",
        "    axs[1].set_ylabel(\"Cumulative Probability\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(out_dir, \"figs\", f\"pdf_cdf_{col}.png\")\n",
        "    plt.savefig(save_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "n = len(df)\n",
        "\n",
        "pd.DataFrame({\"column\": df.columns, \"dtype\": df.dtypes.astype(str)}).to_csv(os.path.join(OUT_DIR,\"columns_types.csv\"), index=False)\n",
        "\n",
        "for col in binary_features:\n",
        "    pmf_df = pmf_with_bootstrap(df[col], n_boot=BOOT_N_PROPORTION)\n",
        "    pmf_csv = os.path.join(OUT_DIR,\"tables\", f\"pmf_{col}.csv\")\n",
        "    pmf_df.to_csv(pmf_csv, index=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    ax.bar(pmf_df['category'].astype(str), pmf_df['pmf'])\n",
        "    ax.errorbar(pmf_df['category'].astype(str), pmf_df['pmf'],\n",
        "                yerr=[pmf_df['pmf']-pmf_df['ci_low'], pmf_df['ci_high']-pmf_df['pmf']],\n",
        "                fmt='none', ecolor='black', capsize=3)\n",
        "    ax.set_title(f\"PMF for {col}\")\n",
        "    ax.set_ylabel(\"Proportion\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    fig_path = os.path.join(OUT_DIR,\"figs\",f\"pmf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "for col in categorical_features:\n",
        "    pmf_df = pmf_with_bootstrap(df[col], n_boot=BOOT_N_PROPORTION)\n",
        "    pmf_csv = os.path.join(OUT_DIR,\"tables\", f\"pmf_{col}.csv\")\n",
        "    pmf_df.to_csv(pmf_csv, index=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.bar(pmf_df['category'].astype(str), pmf_df['pmf'])\n",
        "    ax.errorbar(pmf_df['category'].astype(str), pmf_df['pmf'],\n",
        "                yerr=[pmf_df['pmf']-pmf_df['ci_low'], pmf_df['ci_high']-pmf_df['pmf']],\n",
        "                fmt='none', ecolor='black', capsize=3)\n",
        "    ax.set_title(f\"PMF for {col}\")\n",
        "    ax.set_ylabel(\"Proportion\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    fig_path = os.path.join(OUT_DIR,\"figs\",f\"pmf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "for col in continuous_features:\n",
        "\n",
        "    stats = bootstrap_mean_var_ci(df[col], n_boot=BOOT_N_PROPORTION)\n",
        "    stats_df = pd.DataFrame([{\"stat\":\"mean\",\"estimate\":stats[\"mean\"], \"ci_lo\":stats[\"mean_ci\"][0], \"ci_hi\":stats[\"mean_ci\"][1]},\n",
        "                             {\"stat\":\"var\",\"estimate\":stats[\"var\"], \"ci_lo\":stats[\"var_ci\"][0], \"ci_hi\":stats[\"var_ci\"][1]}])\n",
        "    stats_csv = os.path.join(OUT_DIR,\"tables\", f\"desc_{col}.csv\")\n",
        "    stats_df.to_csv(stats_csv, index=False)\n",
        "\n",
        "    try:\n",
        "        grid, pdf, pdf_low, pdf_high, cdf, cdf_low, cdf_high = kde_pdf_cdf_with_boot(df[col], grid_n=KDE_GRID_N, boot_n=BOOT_N_KDE)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping KDE for {col} due to error:\", e)\n",
        "        continue\n",
        "\n",
        "    pdf_table = pd.DataFrame({\"x\":grid, \"pdf\":pdf, \"pdf_low\":pdf_low, \"pdf_high\":pdf_high, \"cdf\":cdf, \"cdf_low\":cdf_low, \"cdf_high\":cdf_high})\n",
        "    pdf_csv = os.path.join(OUT_DIR,\"tables\", f\"kde_{col}.csv\")\n",
        "    pdf_table.to_csv(pdf_csv, index=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.plot(grid, pdf, label=\"KDE PDF\")\n",
        "    ax.fill_between(grid, pdf_low, pdf_high, alpha=0.3, label=\"95% CI\")\n",
        "    ax.set_title(f\"KDE PDF — {col}\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.legend()\n",
        "    figpath1 = os.path.join(OUT_DIR,\"figs\", f\"kde_pdf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath1, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.plot(grid, cdf, label=\"CDF (from KDE)\")\n",
        "    ax.fill_between(grid, cdf_low, cdf_high, alpha=0.25, label=\"95% CI\")\n",
        "    ax.set_title(f\"CDF — {col}\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Cumulative probability\")\n",
        "    ax.legend()\n",
        "    figpath2 = os.path.join(OUT_DIR,\"figs\", f\"kde_cdf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath2, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md_lines = []\n",
        "md_lines.append(\"# Task 1 — Marginal Distributions\")\n",
        "md_lines.append(f\"Dataset: {os.path.basename(CSV_PATH)}  \\nInstances: {n}\\n\")\n",
        "md_lines.append(\"## Discrete / Categorical variables (PMFs saved to tables/ and plots to figs/)\\n\")\n",
        "for col in binary_features + categorical_features:\n",
        "    pmf_csv = os.path.join(\"tables\", f\"pmf_{col}.csv\")\n",
        "    img = os.path.join(\"figs\", f\"pmf_{col}.png\")\n",
        "    md_lines.append(f\"### {col}\\nSupport: {sorted(df[col].dropna().unique().tolist(), key=lambda x:str(x))}\\n\")\n",
        "    md_lines.append(f\"PMF table: `{pmf_csv}`  \\nFigure: `{img}`\\n\")\n",
        "md_lines.append(\"\\n## Continuous variables (KDE + mean/var bootstraps)\\n\")\n",
        "for col in continuous_features:\n",
        "    desc = os.path.join(\"tables\", f\"desc_{col}.csv\")\n",
        "    kpdf = os.path.join(\"tables\", f\"kde_{col}.csv\")\n",
        "    img1 = os.path.join(\"figs\", f\"kde_pdf_{col}.png\")\n",
        "    img2 = os.path.join(\"figs\", f\"kde_cdf_{col}.png\")\n",
        "    md_lines.append(f\"### {col}\\nSupport: [{df[col].min()}, {df[col].max()}]  \\nDesc table: `{desc}`  \\nKDE table: `{kpdf}`  \\nFigures: `{img1}`, `{img2}`\\n\")\n",
        "md_path = os.path.join(OUT_DIR, \"task1_report.md\")\n",
        "with open(md_path, \"w\") as f:\n",
        "    f.write(\"\\n\".join(md_lines))\n",
        "\n",
        "print(\"Task 1 outputs saved to\", OUT_DIR)\n",
        "print(\"Open\", md_path, \"for a skeleton report (tables and figures saved in tables/ and figs/).\")"
      ],
      "metadata": {
        "id": "7-3_5_mdV_xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oRkjOIxZWJWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4TogIY_AWTzS"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "BOOT_N_PROPORTION = 2000\n",
        "BOOT_N_KDE = 500\n",
        "KDE_GRID_N = 500\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR,\"figs\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR,\"tables\"), exist_ok=True)\n",
        "\n",
        "def bootstrap_ci_values(data, statfunc=np.mean, n_boot=1000, ci=95):\n",
        "    data = np.array(data)\n",
        "    n = len(data)\n",
        "    boots = []\n",
        "    for _ in range(n_boot):\n",
        "        s = np.random.choice(data, size=n, replace=True)\n",
        "        boots.append(statfunc(s))\n",
        "    lo = np.percentile(boots, (100-ci)/2)\n",
        "    hi = np.percentile(boots, 100-(100-ci)/2)\n",
        "    return lo, hi, boots\n",
        "\n",
        "def pmf_with_bootstrap(series, n_boot=1000):\n",
        "    vals = series.dropna()\n",
        "    counts = vals.value_counts().sort_index()\n",
        "    pmf = (counts / counts.sum()).astype(float)\n",
        "    rows = []\n",
        "    for cat in pmf.index:\n",
        "        binary = (vals == cat).astype(int).values\n",
        "        lo, hi, _ = bootstrap_ci_values(binary, statfunc=np.mean, n_boot=n_boot)\n",
        "        rows.append({\"category\": cat, \"pmf\": pmf[cat], \"ci_low\": lo, \"ci_high\": hi, \"count\": int(counts[cat])})\n",
        "    df_pmf = pd.DataFrame(rows).sort_values(\"pmf\", ascending=False)\n",
        "    return df_pmf\n",
        "\n",
        "def kde_pdf_cdf_with_boot(series, grid_n=500, boot_n=500):\n",
        "    data = series.dropna().values\n",
        "    if len(data) < 3:\n",
        "        raise ValueError(\"Not enough data for KDE\")\n",
        "    grid = np.linspace(data.min(), data.max(), grid_n)\n",
        "    kde = gaussian_kde(data)\n",
        "    pdf = kde(grid)\n",
        "    from numpy import trapz\n",
        "    pdf = pdf / trapz(pdf, grid)\n",
        "    cdf = np.cumsum(pdf)\n",
        "    cdf = cdf / cdf[-1]\n",
        "\n",
        "    pdf_boot = np.empty((boot_n, grid_n))\n",
        "    cdf_boot = np.empty((boot_n, grid_n))\n",
        "    for i in range(boot_n):\n",
        "        samp = np.random.choice(data, size=len(data), replace=True)\n",
        "        kv = gaussian_kde(samp)\n",
        "        p = kv(grid)\n",
        "        p = p / trapz(p, grid)\n",
        "        pdf_boot[i,:] = p\n",
        "        c = np.cumsum(p); c = c / c[-1]\n",
        "        cdf_boot[i,:] = c\n",
        "    pdf_low = np.percentile(pdf_boot, 2.5, axis=0)\n",
        "    pdf_high = np.percentile(pdf_boot, 97.5, axis=0)\n",
        "    cdf_low = np.percentile(cdf_boot, 2.5, axis=0)\n",
        "    cdf_high = np.percentile(cdf_boot, 97.5, axis=0)\n",
        "    return grid, pdf, pdf_low, pdf_high, cdf, cdf_low, cdf_high\n",
        "\n",
        "def bootstrap_mean_var_ci(series, n_boot=2000):\n",
        "    arr = np.array(series.dropna())\n",
        "    lo_m, hi_m, _ = bootstrap_ci_values(arr, statfunc=np.mean, n_boot=n_boot)\n",
        "    lo_v, hi_v, _ = bootstrap_ci_values(arr, statfunc=lambda x: x.var(ddof=1), n_boot=n_boot)\n",
        "    return {\"mean\": arr.mean(), \"mean_ci\": (lo_m, hi_m), \"var\": arr.var(ddof=1), \"var_ci\": (lo_v, hi_v)}\n",
        "\n",
        "def plot_pdf_cdf_combined(col, grid, pdf, pdf_low, pdf_high, cdf, cdf_low, cdf_high, out_dir):\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12,4))\n",
        "\n",
        "    # PDF\n",
        "    axs[0].plot(grid, pdf, label=\"KDE PDF\")\n",
        "    axs[0].fill_between(grid, pdf_low, pdf_high, alpha=0.3, label=\"95% CI\")\n",
        "    axs[0].set_title(f\"PDF (KDE) — {col}\")\n",
        "    axs[0].set_xlabel(col)\n",
        "    axs[0].set_ylabel(\"Density\")\n",
        "    axs[0].legend()\n",
        "\n",
        "    # CDF\n",
        "    axs[1].plot(grid, cdf, label=\"CDF\")\n",
        "    axs[1].fill_between(grid, cdf_low, cdf_high, alpha=0.3, label=\"95% CI\")\n",
        "    axs[1].set_title(f\"CDF (KDE) — {col}\")\n",
        "    axs[1].set_xlabel(col)\n",
        "    axs[1].set_ylabel(\"Cumulative Probability\")\n",
        "    axs[1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    save_path = os.path.join(out_dir, \"figs\", f\"pdf_cdf_{col}.png\")\n",
        "    plt.savefig(save_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "n = len(df)\n",
        "\n",
        "pd.DataFrame({\"column\": df.columns, \"dtype\": df.dtypes.astype(str)}).to_csv(os.path.join(OUT_DIR,\"columns_types.csv\"), index=False)\n",
        "\n",
        "for col in binary_features:\n",
        "    pmf_df = pmf_with_bootstrap(df[col], n_boot=BOOT_N_PROPORTION)\n",
        "    pmf_csv = os.path.join(OUT_DIR,\"tables\", f\"pmf_{col}.csv\")\n",
        "    pmf_df.to_csv(pmf_csv, index=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6,4))\n",
        "    ax.bar(pmf_df['category'].astype(str), pmf_df['pmf'])\n",
        "    ax.errorbar(pmf_df['category'].astype(str), pmf_df['pmf'],\n",
        "                yerr=[pmf_df['pmf']-pmf_df['ci_low'], pmf_df['ci_high']-pmf_df['pmf']],\n",
        "                fmt='none', ecolor='black', capsize=3)\n",
        "    ax.set_title(f\"PMF for {col}\")\n",
        "    ax.set_ylabel(\"Proportion\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    fig_path = os.path.join(OUT_DIR,\"figs\",f\"pmf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "for col in categorical_features:\n",
        "    pmf_df = pmf_with_bootstrap(df[col], n_boot=BOOT_N_PROPORTION)\n",
        "    pmf_csv = os.path.join(OUT_DIR,\"tables\", f\"pmf_{col}.csv\")\n",
        "    pmf_df.to_csv(pmf_csv, index=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.bar(pmf_df['category'].astype(str), pmf_df['pmf'])\n",
        "    ax.errorbar(pmf_df['category'].astype(str), pmf_df['pmf'],\n",
        "                yerr=[pmf_df['pmf']-pmf_df['ci_low'], pmf_df['ci_high']-pmf_df['pmf']],\n",
        "                fmt='none', ecolor='black', capsize=3)\n",
        "    ax.set_title(f\"PMF for {col}\")\n",
        "    ax.set_ylabel(\"Proportion\")\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    fig_path = os.path.join(OUT_DIR,\"figs\",f\"pmf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(fig_path, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "for col in continuous_features:\n",
        "\n",
        "    stats = bootstrap_mean_var_ci(df[col], n_boot=BOOT_N_PROPORTION)\n",
        "    stats_df = pd.DataFrame([{\"stat\":\"mean\",\"estimate\":stats[\"mean\"], \"ci_lo\":stats[\"mean_ci\"][0], \"ci_hi\":stats[\"mean_ci\"][1]},\n",
        "                             {\"stat\":\"var\",\"estimate\":stats[\"var\"], \"ci_lo\":stats[\"var_ci\"][0], \"ci_hi\":stats[\"var_ci\"][1]}])\n",
        "    stats_csv = os.path.join(OUT_DIR,\"tables\", f\"desc_{col}.csv\")\n",
        "    stats_df.to_csv(stats_csv, index=False)\n",
        "\n",
        "    try:\n",
        "        grid, pdf, pdf_low, pdf_high, cdf, cdf_low, cdf_high = kde_pdf_cdf_with_boot(df[col], grid_n=KDE_GRID_N, boot_n=BOOT_N_KDE)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping KDE for {col} due to error:\", e)\n",
        "        continue\n",
        "\n",
        "    pdf_table = pd.DataFrame({\"x\":grid, \"pdf\":pdf, \"pdf_low\":pdf_low, \"pdf_high\":pdf_high, \"cdf\":cdf, \"cdf_low\":cdf_low, \"cdf_high\":cdf_high})\n",
        "    pdf_csv = os.path.join(OUT_DIR,\"tables\", f\"kde_{col}.csv\")\n",
        "    pdf_table.to_csv(pdf_csv, index=False)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.plot(grid, pdf, label=\"KDE PDF\")\n",
        "    ax.fill_between(grid, pdf_low, pdf_high, alpha=0.3, label=\"95% CI\")\n",
        "    ax.set_title(f\"KDE PDF — {col}\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    ax.legend()\n",
        "    figpath1 = os.path.join(OUT_DIR,\"figs\", f\"kde_pdf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath1, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(7,4))\n",
        "    ax.plot(grid, cdf, label=\"CDF (from KDE)\")\n",
        "    ax.fill_between(grid, cdf_low, cdf_high, alpha=0.25, label=\"95% CI\")\n",
        "    ax.set_title(f\"CDF — {col}\")\n",
        "    ax.set_xlabel(col)\n",
        "    ax.set_ylabel(\"Cumulative probability\")\n",
        "    ax.legend()\n",
        "    figpath2 = os.path.join(OUT_DIR,\"figs\", f\"kde_cdf_{col}.png\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(figpath2, dpi=200)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "master_rows = []\n",
        "\n",
        "for col in binary_features + categorical_features:\n",
        "    pmf_df = pd.read_csv(os.path.join(OUT_DIR, \"tables\", f\"pmf_{col}.csv\"))\n",
        "    for _, row in pmf_df.iterrows():\n",
        "        master_rows.append({\n",
        "            \"Feature\": col,\n",
        "            \"Type\": \"binary\" if col in binary_features else \"categorical\",\n",
        "            \"Value\": row[\"category\"],\n",
        "            \"PMF\": row[\"pmf\"],\n",
        "            \"CDF\": pmf_df[\"pmf\"].loc[:_].sum()  # cumulative\n",
        "        })\n",
        "\n",
        "for col in continuous_features:\n",
        "    kde_path = os.path.join(OUT_DIR, \"tables\", f\"kde_{col}.csv\")\n",
        "\n",
        "    if not os.path.exists(kde_path):\n",
        "        continue\n",
        "\n",
        "    kde_df = pd.read_csv(kde_path)\n",
        "\n",
        "    for _, row in kde_df.iterrows():\n",
        "        master_rows.append({\n",
        "            \"Feature\": col,\n",
        "            \"Type\": \"continuous\",\n",
        "            \"Value\": row[\"x\"],\n",
        "            \"PMF\": row[\"pdf\"],\n",
        "            \"CDF\": row[\"cdf\"]\n",
        "        })\n",
        "\n",
        "master_table = pd.DataFrame(master_rows)\n",
        "\n",
        "master_csv_path = os.path.join(OUT_DIR, \"tables\", \"task1_master_distribution_table.csv\")\n",
        "master_table.to_csv(master_csv_path, index=False)\n",
        "\n",
        "print(\"Unified master distribution table saved to:\", master_csv_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzgJ7DV6xhKg"
      },
      "source": [
        "# Second task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ys1TsqF-xk-c"
      },
      "outputs": [],
      "source": [
        "\n",
        "map4 = {\n",
        " 'Insufficient_Weight': 'Insufficient Weight',\n",
        " 'Normal_Weight': 'Normal Weight',\n",
        " 'Overweight_Level_I': 'Overweight',\n",
        " 'Overweight_Level_II': 'Overweight',\n",
        " 'Obesity_Type_I': 'Obesity',\n",
        " 'Obesity_Type_II': 'Obesity',\n",
        " 'Obesity_Type_III': 'Obesity'\n",
        "}\n",
        "\n",
        "target_orig = \"NObeyesdad\"\n",
        "target_new = \"NObeyesdad_new\"\n",
        "\n",
        "BOOT_N = 2000\n",
        "ALPHA = 0.05\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR,\"figs2\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUT_DIR,\"tables2\"), exist_ok=True)\n",
        "\n",
        "def bootstrap_ci(data, statfunc, n_boot=2000, ci=95):\n",
        "    data = np.array(data.dropna())\n",
        "    if len(data) == 0:\n",
        "        return np.nan, np.nan, np.nan, []\n",
        "    boots = []\n",
        "    n = len(data)\n",
        "    for _ in range(n_boot):\n",
        "        s = np.random.choice(data, size=n, replace=True)\n",
        "        boots.append(statfunc(s))\n",
        "    boots = np.array(boots)\n",
        "    lo = np.percentile(boots, (100-ci)/2)\n",
        "    hi = np.percentile(boots, 100-(100-ci)/2)\n",
        "    est = statfunc(data)\n",
        "    return est, lo, hi, boots\n",
        "\n",
        "def welch_ttest(group1, group2):\n",
        "    x1 = np.array(group1.dropna())\n",
        "    x2 = np.array(group2.dropna())\n",
        "    n1 = len(x1); n2 = len(x2)\n",
        "    if n1 < 2 or n2 < 2:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    m1 = x1.mean(); m2 = x2.mean()\n",
        "    s1 = x1.var(ddof=1); s2 = x2.var(ddof=1)\n",
        "    se = np.sqrt(s1/n1 + s2/n2)\n",
        "    t = (m1 - m2) / se\n",
        "    num = (s1/n1 + s2/n2)**2\n",
        "    den = ( (s1**2) / ( (n1**2) * (n1 - 1) ) ) + ( (s2**2) / ( (n2**2) * (n2 - 1) ) )\n",
        "    df = num / den if den != 0 else np.nan\n",
        "    p = stats.t.sf(np.abs(t), df)*2\n",
        "    return t, df, p\n",
        "\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "if target_orig not in df.columns:\n",
        "    raise ValueError(f\"{target_orig} column not found in dataset\")\n",
        "df[target_new] = df[target_orig].map(map4).astype('category')\n",
        "\n",
        "classes = df[target_new].cat.categories.tolist()\n",
        "print(\"Target new classes:\", classes)\n",
        "\n",
        "summary_rows = []\n",
        "anova_rows = []\n",
        "pairwise_rows = []\n",
        "tukey_summaries = {}\n",
        "\n",
        "for col in continuous_features:\n",
        "    print(\"\\n\\n====== Feature:\", col, \"======\")\n",
        "    out_prefix = os.path.join(OUT_DIR, \"tables2\", col)\n",
        "    groups = df.groupby(target_new)[col]\n",
        "    group_stats = []\n",
        "    for lvl, series in groups:\n",
        "        n = series.dropna().shape[0]\n",
        "        mean, mean_lo, mean_hi, mean_boots = bootstrap_ci(series, np.mean, n_boot=BOOT_N, ci=95)\n",
        "        var, var_lo, var_hi, var_boots = bootstrap_ci(series, lambda x: x.var(ddof=1), n_boot=BOOT_N, ci=95)\n",
        "        group_stats.append({\n",
        "            \"feature\": col,\n",
        "            \"level\": lvl,\n",
        "            \"n\": int(n),\n",
        "            \"mean\": float(mean) if not np.isnan(mean) else np.nan,\n",
        "            \"var\": float(var) if not np.isnan(var) else np.nan\n",
        "        })\n",
        "    gs_df = pd.DataFrame(group_stats)\n",
        "    gs_csv = f\"{out_prefix}_group_stats.csv\"\n",
        "    gs_df.to_csv(gs_csv, index=False)\n",
        "    print(\"Saved group stats:\", gs_csv)\n",
        "\n",
        "    for r in group_stats:\n",
        "        summary_rows.append(r)\n",
        "\n",
        "    formula = f\"{col} ~ C({target_new})\"\n",
        "    anova_df = df[[col, target_new]].dropna()\n",
        "    if anova_df.shape[0] < 2:\n",
        "        print(\"Not enough data for ANOVA for\", col)\n",
        "        anova_rows.append({\"feature\":col, \"F\":np.nan, \"p\":np.nan, \"df_between\":np.nan, \"df_within\":np.nan})\n",
        "        continue\n",
        "\n",
        "    model = smf.ols(formula, data=anova_df).fit()\n",
        "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "    try:\n",
        "        F = float(anova_table.loc[f\"C({target_new})\", \"F\"])\n",
        "        p = float(anova_table.loc[f\"C({target_new})\", \"PR(>F)\"])\n",
        "        df_between = int(anova_table.loc[f\"C({target_new})\", \"df\"])\n",
        "        df_within = int(anova_table.loc[\"Residual\", \"df\"])\n",
        "    except Exception as e:\n",
        "        F = np.nan; p = np.nan; df_between = np.nan; df_within = np.nan\n",
        "    print(\"ANOVA result: F = {:.4f}, df_between = {}, df_within = {}, p = {:.4g}\".format(F, df_between, df_within, p))\n",
        "    anova_rows.append({\"feature\":col, \"F\":F, \"p\":p, \"df_between\":df_between, \"df_within\":df_within})\n",
        "\n",
        "    pairwise_results = []\n",
        "    if (not np.isnan(p)) and (p < ALPHA):\n",
        "        levels = anova_df[target_new].unique().tolist()\n",
        "        levels = sorted(levels, key=lambda x: str(x))\n",
        "        pairs = []\n",
        "        for a,b in itertools.combinations(levels, 2):\n",
        "            s1 = anova_df.loc[anova_df[target_new]==a, col].dropna()\n",
        "            s2 = anova_df.loc[anova_df[target_new]==b, col].dropna()\n",
        "            tstat, df_welch, pval = welch_ttest(s1, s2)\n",
        "            pairs.append({\"feature\":col, \"group1\":a, \"group2\":b,\n",
        "                          \"t\": float(tstat) if not np.isnan(tstat) else np.nan,\n",
        "                          \"df_welch\": float(df_welch) if not np.isnan(df_welch) else np.nan,\n",
        "                          \"p_uncorrected\": float(pval) if not np.isnan(pval) else np.nan})\n",
        "        pvals = [pp[\"p_uncorrected\"] for pp in pairs]\n",
        "        reject, pvals_adj, _, _ = multipletests(pvals, alpha=ALPHA, method='fdr_bh')\n",
        "        for i, pp in enumerate(pairs):\n",
        "            pp[\"p_adj_fdr_bh\"] = float(pvals_adj[i])\n",
        "            pp[\"reject_fdr\"] = bool(reject[i])\n",
        "            pairwise_results.append(pp)\n",
        "        pairs_df = pd.DataFrame(pairwise_results)\n",
        "        pairs_csv = f\"{out_prefix}_pairwise_welch_fdr.csv\"\n",
        "        pairs_df.to_csv(pairs_csv, index=False)\n",
        "        print(\"Pairwise Welch tests (with FDR) saved to:\", pairs_csv)\n",
        "    else:\n",
        "        print(\"ANOVA not significant (p >= {}) — skipping pairwise tests for {}\".format(ALPHA, col))\n",
        "\n",
        "    for r in pairwise_results:\n",
        "        pairwise_rows.append(r)\n",
        "    try:\n",
        "        tukey = pairwise_tukeyhsd(endog=anova_df[col], groups=anova_df[target_new], alpha=ALPHA)\n",
        "        tukey_summary = tukey.summary()\n",
        "        tukey_df = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n",
        "        tukey_csv = f\"{out_prefix}_tukey.csv\"\n",
        "        tukey_df.to_csv(tukey_csv, index=False)\n",
        "        tukey_summaries[col] = tukey_df\n",
        "        print(\"Tukey HSD saved to:\", tukey_csv)\n",
        "    except Exception as e:\n",
        "        print(\"Tukey HSD failed for\", col, e)\n",
        "\n",
        "\n",
        "pd.DataFrame(summary_rows).to_csv(os.path.join(OUT_DIR,\"tables2\",\"continuous_group_stats_all.csv\"), index=False)\n",
        "pd.DataFrame(anova_rows).to_csv(os.path.join(OUT_DIR,\"tables2\",\"anova_results.csv\"), index=False)\n",
        "pd.DataFrame(pairwise_rows).to_csv(os.path.join(OUT_DIR,\"tables2\",\"pairwise_welch_fdr_all.csv\"), index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "n98gBqC-aEbE"
      },
      "outputs": [],
      "source": [
        "\n",
        "continuous_features = [\"Age\", \"Height\", \"Weight\", \"NCP\", \"FCVC\", \"CH2O\", \"FAF\",\"TUE\"]\n",
        "\n",
        "def bootstrap_ci(data, statfunc, n_boot=1000, ci=95):\n",
        "    \"\"\"Bootstrap CI for a statistic statfunc (data: 1D array or Series). Returns (est, lo, hi, boots).\"\"\"\n",
        "    arr = np.array(data.dropna()) if hasattr(data, \"dropna\") else np.array(data)\n",
        "    if len(arr) == 0:\n",
        "        return np.nan, np.nan, np.nan, np.array([])\n",
        "    boots = []\n",
        "    n = len(arr)\n",
        "    for _ in range(n_boot):\n",
        "        s = np.random.choice(arr, size=n, replace=True)\n",
        "        boots.append(float(statfunc(s)))\n",
        "    boots = np.array(boots)\n",
        "    est = float(statfunc(arr))\n",
        "    lo = float(np.percentile(boots, (100-ci)/2))\n",
        "    hi = float(np.percentile(boots, 100-(100-ci)/2))\n",
        "    return est, lo, hi, boots\n",
        "\n",
        "def welch_ttest(x, y):\n",
        "    \"\"\"Welch t-test with Welch-Satterthwaite df. Returns t, df, p.\"\"\"\n",
        "    x = np.array(x.dropna()) if hasattr(x, \"dropna\") else np.array(x)\n",
        "    y = np.array(y.dropna()) if hasattr(y, \"dropna\") else np.array(y)\n",
        "    n1 = len(x); n2 = len(y)\n",
        "    if n1 < 2 or n2 < 2:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    m1, m2 = x.mean(), y.mean()\n",
        "    s1, s2 = x.var(ddof=1), y.var(ddof=1)\n",
        "    se = math.sqrt(s1/n1 + s2/n2)\n",
        "    t = (m1 - m2) / se if se>0 else np.nan\n",
        "    num = (s1/n1 + s2/n2)**2\n",
        "    den = 0\n",
        "    if n1 > 1:\n",
        "        den += (s1**2) / ((n1**2)*(n1-1))\n",
        "    if n2 > 1:\n",
        "        den += (s2**2) / ((n2**2)*(n2-1))\n",
        "    df = num / den if den != 0 else np.nan\n",
        "    p = stats.t.sf(abs(t), df)*2 if not np.isnan(t) and not np.isnan(df) else np.nan\n",
        "    return float(t) if not np.isnan(t) else np.nan, float(df) if not np.isnan(df) else np.nan, float(p) if not np.isnan(p) else np.nan\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "n_total = len(df)\n",
        "if target_orig not in df.columns:\n",
        "    raise ValueError(f\"{target_orig} not found in data columns\")\n",
        "df[target_new] = df[target_orig].map(map4)\n",
        "df[target_new] = df[target_new].astype(\"category\")\n",
        "levels_order = ['Insufficient Weight','Normal Weight','Overweight','Obesity']\n",
        "df[target_new] = df[target_new].cat.set_categories(levels_order, ordered=True)\n",
        "\n",
        "group_stats_rows = []\n",
        "anova_rows = []\n",
        "levene_rows = []\n",
        "pairwise_rows = []\n",
        "tukey_rows = []\n",
        "\n",
        "for feat in tqdm(continuous_features, desc=\"Features\"):\n",
        "    for lvl in levels_order:\n",
        "        subset = df.loc[df[target_new]==lvl, feat].dropna()\n",
        "        n = len(subset)\n",
        "        mean, mean_lo, mean_hi, mean_boots = bootstrap_ci(subset, np.mean, n_boot=BOOT_MEAN_VAR, ci=95)\n",
        "        var, var_lo, var_hi, var_boots = bootstrap_ci(subset, lambda x: x.var(ddof=1), n_boot=BOOT_MEAN_VAR, ci=95)\n",
        "        group_stats_rows.append({\n",
        "            \"feature\": feat,\n",
        "            \"level\": lvl,\n",
        "            \"n\": int(n),\n",
        "            \"mean\": mean,\n",
        "            \"mean_lo\": mean_lo,\n",
        "            \"mean_hi\": mean_hi,\n",
        "            \"var\": var,\n",
        "            \"var_lo\": var_lo,\n",
        "            \"var_hi\": var_hi\n",
        "        })\n",
        "    anova_df = df[[feat, target_new]].dropna()\n",
        "    if anova_df.shape[0] < 3:\n",
        "        anova_rows.append({\"feature\": feat, \"F\": np.nan, \"p\": np.nan, \"df_between\": np.nan, \"df_within\": np.nan})\n",
        "        levene_rows.append({\"feature\":feat, \"W\":np.nan, \"p\":np.nan})\n",
        "        continue\n",
        "\n",
        "    groups_list = [anova_df.loc[anova_df[target_new]==lvl, feat].dropna().values for lvl in levels_order]\n",
        "    groups_for_levene = [g for g in groups_list if len(g)>0]\n",
        "    if len(groups_for_levene) >= 2:\n",
        "        W, p_levene = stats.levene(*groups_for_levene, center='median')\n",
        "    else:\n",
        "        W, p_levene = np.nan, np.nan\n",
        "    levene_rows.append({\"feature\":feat, \"W\":float(W) if not np.isnan(W) else np.nan, \"p\":float(p_levene) if not np.isnan(p_levene) else np.nan})\n",
        "\n",
        "    try:\n",
        "        model = smf.ols(f\"{feat} ~ C({target_new})\", data=anova_df).fit()\n",
        "        anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "        F = float(anova_table.loc[f\"C({target_new})\", \"F\"])\n",
        "        pval = float(anova_table.loc[f\"C({target_new})\", \"PR(>F)\"])\n",
        "        df_between = float(anova_table.loc[f\"C({target_new})\", \"df\"])\n",
        "        df_within = float(anova_table.loc[\"Residual\", \"df\"])\n",
        "    except Exception as e:\n",
        "        F, pval, df_between, df_within = np.nan, np.nan, np.nan, np.nan\n",
        "\n",
        "    anova_rows.append({\"feature\": feat, \"F\": F, \"p\": pval, \"df_between\": df_between, \"df_within\": df_within})\n",
        "\n",
        "    if not np.isnan(pval) and pval < ALPHA:\n",
        "        level_present = [lvl for lvl in levels_order if anova_df.loc[anova_df[target_new]==lvl, feat].dropna().shape[0] >= 2]\n",
        "        pairs = list(itertools.combinations(level_present, 2))\n",
        "        pvals_raw = []\n",
        "        tmp_rows = []\n",
        "        for (a,b) in pairs:\n",
        "            x = anova_df.loc[anova_df[target_new]==a, feat].dropna()\n",
        "            y = anova_df.loc[anova_df[target_new]==b, feat].dropna()\n",
        "            tstat, df_welch, pval_pair = welch_ttest(x,y)\n",
        "            tmp_rows.append({\"feature\":feat, \"group1\":a, \"group2\":b, \"t\":tstat, \"df_welch\":df_welch, \"p_uncorrected\":pval_pair})\n",
        "            pvals_raw.append(pval_pair if not np.isnan(pval_pair) else 1.0)\n",
        "        if len(pvals_raw) > 0:\n",
        "            reject, pvals_fdr, _, _ = multipletests(pvals_raw, alpha=ALPHA, method='fdr_bh')\n",
        "            for i, row in enumerate(tmp_rows):\n",
        "                row[\"p_fdr\"] = float(pvals_fdr[i])\n",
        "                row[\"reject_fdr\"] = bool(reject[i])\n",
        "                pairwise_rows.append(row)\n",
        "        try:\n",
        "            tuk = pairwise_tukeyhsd(endog=anova_df[feat], groups=anova_df[target_new], alpha=ALPHA)\n",
        "            tuk_df = pd.DataFrame(data=tuk._results_table.data[1:], columns=tuk._results_table.data[0])\n",
        "            tuk_df[\"feature\"] = feat\n",
        "            tuk_df_path = os.path.join(OUT_DIR, \"tables2\", f\"tukey_{feat}.csv\")\n",
        "            tuk_df.to_csv(tuk_df_path, index=False)\n",
        "            for _, r in tuk_df.iterrows():\n",
        "                tukey_rows.append({\"feature\":feat, \"group1\":r[\"group1\"], \"group2\":r[\"group2\"], \"meandiff\":r[\"meandiff\"], \"p_adj\":r[\"p-adj\"], \"lower\":r[\"lower\"], \"upper\":r[\"upper\"], \"reject\":r[\"reject\"]})\n",
        "        except Exception as e:\n",
        "            pass\n",
        "    try:\n",
        "        plt.figure(figsize=(8,5))\n",
        "        sns.boxplot(x=target_new, y=feat, data=anova_df, order=[lvl for lvl in levels_order if lvl in anova_df[target_new].unique()])\n",
        "        plt.title(f\"{feat} by {target_new} (ANOVA p = {pval:.3e})\")\n",
        "        plt.xticks(rotation=20)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR,\"figs\", f\"box_{feat}.png\"), dpi=200)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "group_stats_df = pd.DataFrame(group_stats_rows)\n",
        "anova_df_out = pd.DataFrame(anova_rows)\n",
        "levene_df_out = pd.DataFrame(levene_rows)\n",
        "pairwise_df = pd.DataFrame(pairwise_rows)\n",
        "tukey_df_all = pd.DataFrame(tukey_rows)\n",
        "\n",
        "group_stats_df.to_csv(os.path.join(OUT_DIR,\"tables2\",\"task2_group_stats_by_level.csv\"), index=False)\n",
        "anova_df_out.to_csv(os.path.join(OUT_DIR,\"tables2\",\"task2_anova_results.csv\"), index=False)\n",
        "levene_df_out.to_csv(os.path.join(OUT_DIR,\"tables2\",\"task2_levene_results.csv\"), index=False)\n",
        "pairwise_df.to_csv(os.path.join(OUT_DIR,\"tables2\",\"task2_pairwise_welch_fdr.csv\"), index=False)\n",
        "tukey_df_all.to_csv(os.path.join(OUT_DIR,\"tables2\",\"task2_tukey_all.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ySF5jt7qXJKC"
      },
      "outputs": [],
      "source": [
        "\n",
        "master_rows = []\n",
        "for feat in continuous_features:\n",
        "    # gather anova row\n",
        "    an = anova_df_out[anova_df_out[\"feature\"]==feat].to_dict(\"records\")\n",
        "    an = an[0] if len(an)>0 else {}\n",
        "    # add group-level rows as nested info\n",
        "    gs = group_stats_df[group_stats_df[\"feature\"]==feat].sort_values(\"level\").to_dict(\"records\")\n",
        "    master_rows.append({\n",
        "        \"feature\": feat,\n",
        "        \"anova_F\": an.get(\"F\", np.nan),\n",
        "        \"anova_p\": an.get(\"p\", np.nan),\n",
        "        \"anova_df_between\": an.get(\"df_between\", np.nan),\n",
        "        \"anova_df_within\": an.get(\"df_within\", np.nan),\n",
        "        \"levene_W\": float(levene_df_out[levene_df_out[\"feature\"]==feat][\"W\"]) if feat in list(levene_df_out[\"feature\"]) else np.nan,\n",
        "        \"levene_p\": float(levene_df_out[levene_df_out[\"feature\"]==feat][\"p\"]) if feat in list(levene_df_out[\"feature\"]) else np.nan,\n",
        "        \"group_stats\": gs\n",
        "    })\n",
        "\n",
        "master_df = pd.DataFrame(master_rows)\n",
        "master_df.to_json(os.path.join(OUT_DIR,\"tables2\",\"task2_continuous_summary.json\"), orient=\"records\", indent=2)\n",
        "# also write a flattened annex CSV (one row per feature-level)\n",
        "group_stats_df.to_csv(os.path.join(OUT_DIR,\"tables2\",\"task2_continuous_annex_flat.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SWa7FubzEEw"
      },
      "source": [
        "# Third task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0y8aQ7QbzY7f"
      },
      "outputs": [],
      "source": [
        "binary_features = [\n",
        "    \"family_history_with_overweight\",\n",
        "    \"FAVC\",\n",
        "    \"SMOKE\",\n",
        "    \"SCC\",\n",
        "]\n",
        "\n",
        "categorical_features = [\n",
        "    \"Gender\", \"CAEC\", \"CALC\", \"MTRANS\", \"NObeyesdad_new\"\n",
        "]\n",
        "\n",
        "target = \"NObeyesdad_new\"\n",
        "obesity_levels = sorted(df[target].unique())\n",
        "\n",
        "cond_proportions = {}\n",
        "\n",
        "results = []\n",
        "\n",
        "raw_pvalues = []\n",
        "\n",
        "def run_stat_test(contingency, is_binary):\n",
        "\n",
        "    chi2 = np.nan\n",
        "    dof = np.nan\n",
        "    p = np.nan\n",
        "    min_expected = np.nan\n",
        "    test_used = \"N/A\"\n",
        "\n",
        "    if contingency.size == 0 or contingency.shape[0] < 2 or contingency.shape[1] < 2:\n",
        "        return chi2, dof, p, min_expected, test_used\n",
        "\n",
        "    try:\n",
        "        chi2, p, dof, expected = chi2_contingency(contingency)\n",
        "        min_expected = expected.min()\n",
        "        test_used = \"Chi-square\"\n",
        "    except:\n",
        "        if contingency.shape == (2, 2):\n",
        "            try:\n",
        "                _, p = fisher_exact(contingency)\n",
        "                test_used = \"Fisher\"\n",
        "            except:\n",
        "                pass\n",
        "        return chi2, dof, p, min_expected, test_used\n",
        "\n",
        "    if is_binary and contingency.shape == (2,2) and min_expected < 5:\n",
        "        _, p = fisher_exact(contingency)\n",
        "        test_used = \"Fisher\"\n",
        "\n",
        "    return chi2, dof, p, min_expected, test_used\n",
        "\n",
        "for feature in binary_features + categorical_features:\n",
        "\n",
        "    is_binary = feature in binary_features\n",
        "\n",
        "    try:\n",
        "        pmf = (\n",
        "            df.groupby(target)[feature]\n",
        "            .value_counts(normalize=True, dropna=True)\n",
        "            .unstack(fill_value=0)\n",
        "        )\n",
        "    except:\n",
        "        unique_vals = df[feature].dropna().unique()\n",
        "        pmf = pd.DataFrame(\n",
        "            0,\n",
        "            index=obesity_levels,\n",
        "            columns=unique_vals\n",
        "        )\n",
        "    cond_proportions[feature] = pmf\n",
        "\n",
        "    contingency = pd.crosstab(df[target], df[feature])\n",
        "\n",
        "    chi2, dof, p, min_exp, method = run_stat_test(contingency, is_binary)\n",
        "\n",
        "    raw_pvalues.append(p)\n",
        "\n",
        "    results.append({\n",
        "        \"Feature\": feature,\n",
        "        \"Type\": \"Binary\" if is_binary else \"Categorical\",\n",
        "        \"chi2\": chi2,\n",
        "        \"dof\": dof,\n",
        "        \"pvalue\": p,\n",
        "        \"min_expected\": min_exp,\n",
        "        \"Test_used\": method\n",
        "    })\n",
        "\n",
        "cleaned_pvals = [1.0 if pd.isna(v) else v for v in raw_pvalues]\n",
        "reject, pvals_fdr, _, _ = multipletests(cleaned_pvals, method=\"fdr_bh\", alpha=0.05)\n",
        "\n",
        "for i in range(len(results)):\n",
        "    results[i][\"pvalue_fdr\"] = pvals_fdr[i]\n",
        "    results[i][\"reject_H0\"] = reject[i]\n",
        "\n",
        "task3_final_table = pd.DataFrame(results)\n",
        "task3_final_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7afLnR_a0kJx"
      },
      "source": [
        "# Fourth task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JghMJWa7Y-6w"
      },
      "outputs": [],
      "source": [
        "target = \"NObeyesdad_new\"\n",
        "\n",
        "if target not in df.columns:\n",
        "    map4 = {\n",
        "        'Insufficient_Weight': 'Insufficient Weight',\n",
        "        'Normal_Weight': 'Normal Weight',\n",
        "        'Overweight_Level_I': 'Overweight',\n",
        "        'Overweight_Level_II': 'Overweight',\n",
        "        'Obesity_Type_I': 'Obesity',\n",
        "        'Obesity_Type_II': 'Obesity',\n",
        "        'Obesity_Type_III': 'Obesity'\n",
        "    }\n",
        "    df[target] = df[\"NObeyesdad\"].map(map4)\n",
        "\n",
        "levels = [\"Insufficient Weight\",\"Normal Weight\",\"Overweight\",\"Obesity\"]\n",
        "\n",
        "def compute_pmf(series):\n",
        "    counts = series.value_counts().sort_index()\n",
        "    pmf = counts / counts.sum()\n",
        "    cdf = pmf.cumsum()\n",
        "    return pd.DataFrame({\"category\": pmf.index, \"pmf\": pmf.values, \"cdf\": cdf.values})\n",
        "\n",
        "def kde_pdf_cdf(series, grid_n=500):\n",
        "    data = series.dropna().values\n",
        "    if len(data) < 3:\n",
        "        return None\n",
        "    grid = np.linspace(data.min(), data.max(), grid_n)\n",
        "    kde = gaussian_kde(data)\n",
        "    pdf = kde(grid)\n",
        "    pdf /= np.trapz(pdf, grid)\n",
        "    cdf = np.cumsum(pdf)\n",
        "    cdf /= cdf[-1]\n",
        "    return pd.DataFrame({\"x\": grid, \"pdf\": pdf, \"cdf\": cdf})\n",
        "\n",
        "pmf_summary_rows = []\n",
        "cdf_summary_rows = []\n",
        "\n",
        "def plot_all_levels(feature, is_continuous, results_dict, save_path):\n",
        "\n",
        "    colors = {\n",
        "        \"Insufficient Weight\": \"#1f77b4\",\n",
        "        \"Normal Weight\": \"#ff7f0e\",\n",
        "        \"Overweight\": \"#2ca02c\",\n",
        "        \"Obesity\": \"#d62728\"\n",
        "    }\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(14,5))\n",
        "\n",
        "    ax_pdf, ax_cdf = axs\n",
        "\n",
        "    for lvl in levels:\n",
        "        df_plot = results_dict[lvl]\n",
        "\n",
        "        if is_continuous:\n",
        "            # PDF\n",
        "            ax_pdf.plot(\n",
        "                df_plot[\"x\"], df_plot[\"pdf\"],\n",
        "                label=lvl, color=colors[lvl], linewidth=2.2\n",
        "            )\n",
        "            # CDF\n",
        "            ax_cdf.plot(\n",
        "                df_plot[\"x\"], df_plot[\"cdf\"],\n",
        "                label=lvl, color=colors[lvl], linewidth=2.2\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # PMF (discrete)\n",
        "            ax_pdf.plot(\n",
        "                df_plot[\"category\"].astype(str), df_plot[\"pmf\"],\n",
        "                marker=\"o\", markersize=7,\n",
        "                label=lvl, color=colors[lvl], linewidth=2.0\n",
        "            )\n",
        "            # CDF (discrete)\n",
        "            ax_cdf.plot(\n",
        "                df_plot[\"category\"].astype(str), df_plot[\"cdf\"],\n",
        "                marker=\"o\", markersize=7,\n",
        "                label=lvl, color=colors[lvl], linewidth=2.0\n",
        "            )\n",
        "\n",
        "    ax_pdf.set_title(f\"{feature} — PMF/PDF by Obesity Level\", fontsize=13)\n",
        "    ax_cdf.set_title(f\"{feature} — CDF by Obesity Level\", fontsize=13)\n",
        "\n",
        "    ax_pdf.set_xlabel(feature, fontsize=12)\n",
        "    ax_cdf.set_xlabel(feature, fontsize=12)\n",
        "\n",
        "    ax_pdf.set_ylabel(\"Density\", fontsize=12)\n",
        "    ax_cdf.set_ylabel(\"Cumulative Probability\", fontsize=12)\n",
        "\n",
        "    ax_pdf.grid(alpha=0.3)\n",
        "    ax_cdf.grid(alpha=0.3)\n",
        "\n",
        "    handles, labels = ax_pdf.get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc=\"upper center\", ncol=4, fontsize=11)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "for col in tqdm(binary_features + categorical_features):\n",
        "\n",
        "    results_dict = {}\n",
        "\n",
        "    for lvl in levels:\n",
        "        subset = df[df[target] == lvl][col]\n",
        "        pmf_df = compute_pmf(subset)\n",
        "        results_dict[lvl] = pmf_df\n",
        "\n",
        "        for _, row in pmf_df.iterrows():\n",
        "            pmf_summary_rows.append({\n",
        "                \"Feature\": col,\n",
        "                \"Category\": row[\"category\"],\n",
        "                \"Obesity Level\": lvl,\n",
        "                \"PMF\": row[\"pmf\"]\n",
        "            })\n",
        "            cdf_summary_rows.append({\n",
        "                \"Feature\": col,\n",
        "                \"Category\": row[\"category\"],\n",
        "                \"Obesity Level\": lvl,\n",
        "                \"CDF\": row[\"cdf\"]\n",
        "            })\n",
        "\n",
        "    save_path = os.path.join(OUT_DIR, \"figs\", f\"{col}.png\")\n",
        "    plot_all_levels(col, is_continuous=False, results_dict=results_dict, save_path=save_path)\n",
        "\n",
        "continuous_results = {}\n",
        "\n",
        "for col in tqdm(continuous_features):\n",
        "\n",
        "    results_dict = {}\n",
        "    for lvl in levels:\n",
        "        subset = df[df[target] == lvl][col]\n",
        "        kde_df = kde_pdf_cdf(subset)\n",
        "        if kde_df is not None:\n",
        "            results_dict[lvl] = kde_df\n",
        "\n",
        "    save_path = os.path.join(OUT_DIR, \"figs\", f\"{col}.png\")\n",
        "    plot_all_levels(col, is_continuous=True, results_dict=results_dict, save_path=save_path)\n",
        "\n",
        "pmf_df = pd.DataFrame(pmf_summary_rows)\n",
        "cdf_df = pd.DataFrame(cdf_summary_rows)\n",
        "\n",
        "pmf_table = pmf_df.pivot_table(index=[\"Feature\",\"Category\"], columns=\"Obesity Level\", values=\"PMF\").reset_index()\n",
        "cdf_table = cdf_df.pivot_table(index=[\"Feature\",\"Category\"], columns=\"Obesity Level\", values=\"CDF\").reset_index()\n",
        "\n",
        "pmf_table.to_csv(os.path.join(OUT_DIR, \"tables\", \"task4_conditional_pmf_table.csv\"), index=False)\n",
        "cdf_table.to_csv(os.path.join(OUT_DIR, \"tables\", \"task4_conditional_cdf_table.csv\"), index=False)\n",
        "\n",
        "print(\"TASK 4 COMPLETE: Summary tables and figures generated.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOaWUj07iuve"
      },
      "source": [
        "# Fifth task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1UeIzpXRiy7L"
      },
      "outputs": [],
      "source": [
        "\n",
        "continuous_features = [\"Age\", \"Height\", \"Weight\", \"NCP\", \"FCVC\", \"CH2O\", \"FAF\", \"TUE\"]\n",
        "levels = [\"Insufficient Weight\", \"Normal Weight\", \"Obesity\", \"Overweight\"]\n",
        "\n",
        "def kde_model_based_mean_variance(series, grid_points=2000):\n",
        "    x = series.dropna().values\n",
        "    if len(x) < 5:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "    kde = gaussian_kde(x)\n",
        "\n",
        "    x_grid = np.linspace(x.min(), x.max(), grid_points)\n",
        "    pdf_vals = kde(x_grid)\n",
        "\n",
        "    pdf_vals /= np.trapz(pdf_vals, x_grid)\n",
        "\n",
        "    mean_model = np.trapz(x_grid * pdf_vals, x_grid)\n",
        "    var_model = np.trapz((x_grid - mean_model)**2 * pdf_vals, x_grid)\n",
        "\n",
        "    return mean_model, var_model\n",
        "\n",
        "rows = []\n",
        "\n",
        "for feat in continuous_features:\n",
        "    for lvl in levels:\n",
        "        subset = df.loc[df[\"NObeyesdad_new\"] == lvl, feat]\n",
        "\n",
        "        mean_model, var_model = kde_model_based_mean_variance(subset)\n",
        "\n",
        "        mean_sample = task2_lookup.loc[(feat, lvl), \"mean\"]\n",
        "        var_sample  = task2_lookup.loc[(feat, lvl), \"var\"]\n",
        "\n",
        "        tol = 1e-3\n",
        "        mean_match = np.isclose(mean_model, mean_sample, atol=tol)\n",
        "        var_match = np.isclose(var_model, var_sample, atol=tol)\n",
        "\n",
        "        rows.append({\n",
        "            \"feature\": feat,\n",
        "            \"level\": lvl,\n",
        "            \"mean_model\": mean_model,\n",
        "            \"mean_sample\": mean_sample,\n",
        "            \"mean_equivalent\": mean_match,\n",
        "            \"var_model\": var_model,\n",
        "            \"var_sample\": var_sample,\n",
        "            \"var_equivalent\": var_match\n",
        "        })\n",
        "\n",
        "task5_results = pd.DataFrame(rows)\n",
        "task5_results.to_csv(os.path.join(OUT_DIR, \"task5_model_based_comparisons.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu2zKQ3Ymd9y"
      },
      "source": [
        "# Sixth task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SdV3Qps7mfy1"
      },
      "outputs": [],
      "source": [
        "target_orig = \"NObeyesdad\"\n",
        "target_new = \"NObeyesdad_new\"\n",
        "levels_order = [\"Insufficient Weight\",\"Normal Weight\",\"Overweight\",\"Obesity\"]\n",
        "\n",
        "N_BOOT_KDE = 500\n",
        "GRID_N = 800\n",
        "N_BOOT_ROC = 1000\n",
        "ALPHA = 0.05\n",
        "\n",
        "\n",
        "def map_target_to_4(df):\n",
        "    if target_new in df.columns:\n",
        "        return df\n",
        "    map4 = {\n",
        "        'Insufficient_Weight': 'Insufficient Weight',\n",
        "        'Normal_Weight': 'Normal Weight',\n",
        "        'Overweight_Level_I': 'Overweight',\n",
        "        'Overweight_Level_II': 'Overweight',\n",
        "        'Obesity_Type_I': 'Obesity',\n",
        "        'Obesity_Type_II': 'Obesity',\n",
        "        'Obesity_Type_III': 'Obesity'\n",
        "    }\n",
        "    df[target_new] = df[target_orig].map(map4)\n",
        "    df[target_new] = pd.Categorical(df[target_new], categories=levels_order, ordered=True)\n",
        "    return df\n",
        "\n",
        "def kde_fit_and_grid(data, grid):\n",
        "    x = np.asarray(data.dropna())\n",
        "    if len(x) < 3:\n",
        "        return np.zeros_like(grid)\n",
        "    kde = gaussian_kde(x)\n",
        "    pdf = kde(grid)\n",
        "    # normalize\n",
        "    pdf = pdf / np.trapz(pdf, grid)\n",
        "    return pdf\n",
        "\n",
        "def bootstrap_kde_bands(data, grid, n_boot=N_BOOT_KDE):\n",
        "    x = np.asarray(data.dropna())\n",
        "    if len(x) < 3:\n",
        "        return np.zeros_like(grid), np.zeros_like(grid), np.zeros_like(grid)\n",
        "    pdfs = np.empty((n_boot, len(grid)))\n",
        "    n = len(x)\n",
        "    for i in range(n_boot):\n",
        "        samp = np.random.choice(x, size=n, replace=True)\n",
        "        try:\n",
        "            pdfs[i,:] = gaussian_kde(samp)(grid)\n",
        "        except Exception:\n",
        "            pdfs[i,:] = np.zeros_like(grid)\n",
        "    # normalize each\n",
        "    for i in range(n_boot):\n",
        "        arr = pdfs[i,:]\n",
        "        s = np.trapz(arr, grid)\n",
        "        if s > 0:\n",
        "            pdfs[i,:] = arr / s\n",
        "    pdf_mean = np.mean(pdfs, axis=0)\n",
        "    pdf_low = np.percentile(pdfs, 2.5, axis=0)\n",
        "    pdf_high = np.percentile(pdfs, 97.5, axis=0)\n",
        "    return pdf_mean, pdf_low, pdf_high\n",
        "\n",
        "def find_discriminative_intervals(grid, pdf_low_g, pdf_high_g, other_pdfs_low, other_pdfs_high, min_width=0.01):\n",
        "    other_high_max = np.maximum.reduce(other_pdfs_high) if len(other_pdfs_high)>0 else np.full_like(grid, -np.inf)\n",
        "    other_low_min = np.minimum.reduce(other_pdfs_low) if len(other_pdfs_low)>0 else np.full_like(grid, np.inf)\n",
        "\n",
        "    cond_higher = pdf_low_g > other_high_max\n",
        "    cond_lower = pdf_high_g < other_low_min\n",
        "\n",
        "    def grid_to_intervals(cond_mask):\n",
        "        intervals = []\n",
        "        i = 0\n",
        "        while i < len(cond_mask):\n",
        "            if cond_mask[i]:\n",
        "                j = i\n",
        "                while j+1 < len(cond_mask) and cond_mask[j+1]:\n",
        "                    j += 1\n",
        "                a = grid[i]\n",
        "                b = grid[j]\n",
        "                if (b - a) >= min_width*(grid.max()-grid.min()):\n",
        "                    intervals.append((a, b))\n",
        "                i = j+1\n",
        "            else:\n",
        "                i += 1\n",
        "        return intervals\n",
        "\n",
        "    higher_ints = grid_to_intervals(cond_higher)\n",
        "    lower_ints = grid_to_intervals(cond_lower)\n",
        "    return higher_ints, lower_ints\n",
        "\n",
        "def youden_threshold(y_true, scores):\n",
        "    fpr, tpr, thr = roc_curve(y_true, scores)\n",
        "    J = tpr - fpr\n",
        "    idx = np.argmax(J)\n",
        "    return thr[idx], fpr[idx], 1 - (1 - tpr[idx])\n",
        "\n",
        "def bootstrap_auc_ci(y_true, scores, n_boot=N_BOOT_ROC, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(y_true)\n",
        "    aucs = []\n",
        "    for i in range(n_boot):\n",
        "        idx = rng.choice(np.arange(n), size=n, replace=True)\n",
        "        if len(np.unique(y_true[idx])) < 2:\n",
        "            aucs.append(np.nan)\n",
        "        else:\n",
        "            aucs.append(roc_auc_score(y_true[idx], scores[idx]))\n",
        "    aucs = np.array(aucs)\n",
        "    lo = np.nanpercentile(aucs, 2.5)\n",
        "    hi = np.nanpercentile(aucs, 97.5)\n",
        "    return np.nanmean(aucs), lo, hi, aucs\n",
        "\n",
        "def bootstrap_rates_at_threshold(y_true, scores, threshold, n_boot=N_BOOT_ROC, seed=42):\n",
        "    rng = np.random.RandomState(seed)\n",
        "    n = len(y_true)\n",
        "    fprs = []\n",
        "    fnrs = []\n",
        "    for i in range(n_boot):\n",
        "        idx = rng.choice(np.arange(n), size=n, replace=True)\n",
        "        yt = y_true[idx]\n",
        "        sc = scores[idx]\n",
        "        yhat = (sc >= threshold).astype(int)\n",
        "\n",
        "        tn, fp, fn, tp = confusion_matrix(yt, yhat, labels=[0,1]).ravel()\n",
        "        fpr = fp / (fp + tn) if (fp + tn) > 0 else np.nan\n",
        "        fnr = fn / (fn + tp) if (fn + tp) > 0 else np.nan\n",
        "        fprs.append(fpr)\n",
        "        fnrs.append(fnr)\n",
        "    return np.nanpercentile(fprs, 2.5), np.nanpercentile(fprs,97.5), np.nanpercentile(fnrs,2.5), np.nanpercentile(fnrs,97.5), np.nanmean(fprs), np.nanmean(fnrs)\n",
        "\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "df = map_target_to_4(df)\n",
        "df = df.dropna(subset=[target_new])\n",
        "\n",
        "rows_discriminative = []\n",
        "rows_ks = []\n",
        "rows_roc = []\n",
        "\n",
        "power_records = []\n",
        "alpha_records = []\n",
        "\n",
        "for feat in tqdm(continuous_features, desc=\"Features\"):\n",
        "    all_vals = df[feat].dropna().values\n",
        "    if len(all_vals) < 3:\n",
        "        print(\"Skipping\", feat, \"not enough data\")\n",
        "        continue\n",
        "    grid = np.linspace(all_vals.min(), all_vals.max(), GRID_N)\n",
        "\n",
        "    bands = {}\n",
        "    for lvl in levels_order:\n",
        "        data_lvl = df.loc[df[target_new]==lvl, feat]\n",
        "        mean_pdf, low_pdf, high_pdf = bootstrap_kde_bands(data_lvl, grid, n_boot=N_BOOT_KDE)\n",
        "        bands[lvl] = {\"mean\": mean_pdf, \"low\": low_pdf, \"high\": high_pdf}\n",
        "\n",
        "    for lvl in levels_order:\n",
        "        pdf_mean_g = bands[lvl][\"mean\"]\n",
        "        pdf_low_g = bands[lvl][\"low\"]\n",
        "        pdf_high_g = bands[lvl][\"high\"]\n",
        "        # others\n",
        "        other_lows = []\n",
        "        other_highs = []\n",
        "        for other in levels_order:\n",
        "            if other == lvl:\n",
        "                continue\n",
        "            other_lows.append(bands[other][\"low\"])\n",
        "            other_highs.append(bands[other][\"high\"])\n",
        "\n",
        "        higher_ints, lower_ints = find_discriminative_intervals(grid, pdf_low_g, pdf_high_g, other_lows, other_highs, min_width=0.01)\n",
        "\n",
        "        ks_results = []\n",
        "        for other in levels_order:\n",
        "            if other==lvl: continue\n",
        "            x1 = df.loc[df[target_new]==lvl, feat].dropna().values\n",
        "            x2 = df.loc[df[target_new]==other, feat].dropna().values\n",
        "            if len(x1) < 2 or len(x2) <2:\n",
        "                pval = np.nan\n",
        "                stat = np.nan\n",
        "            else:\n",
        "                stat, pval = ks_2samp(x1, x2)\n",
        "            ks_results.append({\"feature\":feat, \"level\":lvl, \"other\":other, \"ks_stat\":float(stat), \"pvalue\":float(pval)})\n",
        "            rows_ks.append(ks_results[-1])\n",
        "\n",
        "        rows_discriminative.append({\n",
        "            \"feature\":feat,\n",
        "            \"level\":lvl,\n",
        "            \"intervals_higher\": higher_ints,\n",
        "            \"intervals_lower\": lower_ints,\n",
        "            \"n_higher_intervals\": len(higher_ints),\n",
        "            \"n_lower_intervals\": len(lower_ints)\n",
        "        })\n",
        "\n",
        "    for lvl in levels_order:\n",
        "        y_true = (df[target_new] == lvl).astype(int).values\n",
        "        scores = df[feat].fillna(df[feat].median()).values\n",
        "\n",
        "        try:\n",
        "            auc_full = roc_auc_score(y_true, scores)\n",
        "        except Exception:\n",
        "            auc_full = np.nan\n",
        "\n",
        "        try:\n",
        "            thr, fpr_at_thr, tpr_at_thr = youden_threshold(y_true, scores)\n",
        "        except Exception:\n",
        "            thr, fpr_at_thr, tpr_at_thr = np.nan, np.nan, np.nan\n",
        "\n",
        "        yhat = (scores >= thr).astype(int)\n",
        "        try:\n",
        "            tn, fp, fn, tp = confusion_matrix(y_true, yhat, labels=[0,1]).ravel()\n",
        "        except Exception:\n",
        "            tn=fp=fn=tp = np.nan\n",
        "\n",
        "        mean_auc_boot, auc_lo, auc_hi, aucs = bootstrap_auc_ci(y_true, scores, n_boot=N_BOOT_ROC, seed=42)\n",
        "\n",
        "        fpr_lo, fpr_hi, fnr_lo, fnr_hi, fpr_mean, fnr_mean = bootstrap_rates_at_threshold(y_true, scores, thr, n_boot=N_BOOT_ROC, seed=42)\n",
        "\n",
        "        # store\n",
        "        rows_roc.append({\n",
        "            \"feature\": feat,\n",
        "            \"level\": lvl,\n",
        "            \"n_pos\": int(y_true.sum()),\n",
        "            \"n_neg\": int(len(y_true)-y_true.sum()),\n",
        "            \"auc\": float(auc_full) if not np.isnan(auc_full) else np.nan,\n",
        "            \"auc_boot_mean\": float(mean_auc_boot),\n",
        "            \"auc_ci_lo\": float(auc_lo),\n",
        "            \"auc_ci_hi\": float(auc_hi),\n",
        "            \"youden_threshold\": float(thr) if not np.isnan(thr) else np.nan,\n",
        "            \"fpr_at_threshold\": float(fpr_at_thr) if not np.isnan(fpr_at_thr) else np.nan,\n",
        "            \"tpr_at_threshold\": float(tpr_at_thr) if not np.isnan(tpr_at_thr) else np.nan,\n",
        "            \"confusion_tn\": int(tn) if not np.isnan(tn) else np.nan,\n",
        "            \"confusion_fp\": int(fp) if not np.isnan(fp) else np.nan,\n",
        "            \"confusion_fn\": int(fn) if not np.isnan(fn) else np.nan,\n",
        "            \"confusion_tp\": int(tp) if not np.isnan(tp) else np.nan,\n",
        "            \"fpr_boot_lo\": float(fpr_lo),\n",
        "            \"fpr_boot_hi\": float(fpr_hi),\n",
        "            \"fnr_boot_lo\": float(fnr_lo),\n",
        "            \"fnr_boot_hi\": float(fnr_hi),\n",
        "            \"fpr_boot_mean\": float(fpr_mean),\n",
        "            \"fnr_boot_mean\": float(fnr_mean)\n",
        "        })\n",
        "\n",
        "        power_records.append({\"feature\":feat, \"level\":lvl, \"power\": 1 - float(fnr_mean)})\n",
        "        alpha_records.append({\"feature\":feat, \"level\":lvl, \"alpha\": float(fpr_mean)})\n",
        "\n",
        "    if lvl == levels_order[-1]:\n",
        "        plt.figure(figsize=(7,6))\n",
        "\n",
        "        for lvl2 in levels_order:\n",
        "            y_true2 = (df[target_new] == lvl2).astype(int).values\n",
        "            scores2 = df[feat].fillna(df[feat].median()).values\n",
        "\n",
        "            if len(np.unique(y_true2)) < 2:\n",
        "                continue\n",
        "\n",
        "            fpr_full, tpr_full, thr_full = roc_curve(y_true2, scores2)\n",
        "            try:\n",
        "                auc_full_lvl = roc_auc_score(y_true2, scores2)\n",
        "            except:\n",
        "                auc_full_lvl = np.nan\n",
        "\n",
        "            plt.plot(\n",
        "                fpr_full,\n",
        "                tpr_full,\n",
        "                linewidth=2,\n",
        "                label=f\"{lvl2} (AUC = {auc_full_lvl:.3f})\"\n",
        "            )\n",
        "\n",
        "        plt.plot([0,1], [0,1], 'k--', alpha=0.5)\n",
        "        plt.title(f\"Unified ROC Curve — {feat}\", fontsize=14)\n",
        "        plt.xlabel(\"False Positive Rate\", fontsize=12)\n",
        "        plt.ylabel(\"True Positive Rate\", fontsize=12)\n",
        "        plt.legend(title=\"Obesity Level\", fontsize=10)\n",
        "        plt.grid(alpha=0.3)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(OUT_DIR,\"figs\",f\"roc_unified_{feat}.png\"), dpi=200)\n",
        "        plt.show()\n",
        "\n",
        "pd.DataFrame(rows_discriminative).to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_discriminative_intervals.csv\"), index=False)\n",
        "pd.DataFrame(rows_ks).to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_ks_pairwise.csv\"), index=False)\n",
        "pd.DataFrame(rows_roc).to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_roc_bootstrap_results.csv\"), index=False)\n",
        "pd.DataFrame(power_records).to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_power_records.csv\"), index=False)\n",
        "pd.DataFrame(alpha_records).to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_alpha_records.csv\"), index=False)\n",
        "\n",
        "power_df = pd.DataFrame(power_records)\n",
        "alpha_df = pd.DataFrame(alpha_records)\n",
        "power_summary = power_df.groupby(\"feature\")[\"power\"].mean().reset_index().sort_values(\"power\", ascending=False)\n",
        "power_summary.to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_power_summary.csv\"), index=False)\n",
        "alpha_summary = alpha_df.groupby(\"feature\")[\"alpha\"].mean().reset_index().sort_values(\"alpha\")\n",
        "alpha_summary.to_csv(os.path.join(OUT_DIR,\"tables\",\"task6_alpha_summary.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh8virAk_neV"
      },
      "source": [
        "# CORRECTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ3kX3CeQt-o"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ANOVA with assumption checks + post-hoc\n",
        "\n",
        "map4 = {\n",
        " 'Insufficient_Weight': 'Insufficient Weight',\n",
        " 'Normal_Weight': 'Normal Weight',\n",
        " 'Overweight_Level_I': 'Overweight',\n",
        " 'Overweight_Level_II': 'Overweight',\n",
        " 'Obesity_Type_I': 'Obesity',\n",
        " 'Obesity_Type_II': 'Obesity',\n",
        " 'Obesity_Type_III': 'Obesity'\n",
        "}\n",
        "if 'NObeyesdad' in df.columns and 'NObeyesdad_new' not in df.columns:\n",
        "    df['NObeyesdad_new'] = df['NObeyesdad'].map(map4)\n",
        "df = df.dropna(subset=['NObeyesdad_new'])\n",
        "\n",
        "continuous_features = [\"Age\",\"Height\",\"Weight\",\"NCP\",\"FCVC\",\"CH2O\",\"FAF\",\"TUE\"]\n",
        "alpha = 0.05\n",
        "\n",
        "anova_summary_rows = []\n",
        "assumption_rows = []\n",
        "posthoc_rows = []\n",
        "welch_pairwise_rows = []\n",
        "\n",
        "for feat in tqdm(continuous_features):\n",
        "    sub = df[[feat,'NObeyesdad_new']].dropna()\n",
        "    if sub.shape[0] < 10:\n",
        "        continue\n",
        "\n",
        "    formula = f\"{feat} ~ C(NObeyesdad_new)\"\n",
        "    model = smf.ols(formula, data=sub).fit()\n",
        "    anova_table = sm.stats.anova_lm(model, typ=2)\n",
        "    F = anova_table.loc['C(NObeyesdad_new)','F'] if 'C(NObeyesdad_new)' in anova_table.index else np.nan\n",
        "    pval = anova_table.loc['C(NObeyesdad_new)','PR(>F)'] if 'C(NObeyesdad_new)' in anova_table.index else np.nan\n",
        "\n",
        "    groups = [g[feat].values for n,g in sub.groupby('NObeyesdad_new')]\n",
        "    try:\n",
        "        f_sc, p_sc = stats.f_oneway(*groups)\n",
        "    except Exception:\n",
        "        f_sc, p_sc = np.nan, np.nan\n",
        "\n",
        "    anova_summary_rows.append({'feature':feat,'F_lm':float(F),'p_lm':float(pval),'F_foneway':float(f_sc),'p_foneway':float(p_sc)})\n",
        "\n",
        "    resid = model.resid\n",
        "    fitted = model.fittedvalues\n",
        "\n",
        "    try:\n",
        "        sh_stat, sh_p = stats.shapiro(resid)\n",
        "    except Exception:\n",
        "        sh_stat, sh_p = np.nan, np.nan\n",
        "\n",
        "    groups_for_levene = [grp[feat].values for name,grp in sub.groupby('NObeyesdad_new')]\n",
        "    try:\n",
        "        lev_stat, lev_p = stats.levene(*groups_for_levene, center='median')\n",
        "    except Exception:\n",
        "        lev_stat, lev_p = np.nan, np.nan\n",
        "\n",
        "    assumption_rows.append({'feature':feat,'shapiro_stat':float(sh_stat),'shapiro_p':float(sh_p),'levene_stat':float(lev_stat),'levene_p':float(lev_p)})\n",
        "\n",
        "    qqplot(resid, line='s')\n",
        "    plt.title(f\"QQ plot residuals — {feat}\")\n",
        "    plt.savefig(os.path.join(OUT_DIR,f\"qq_{feat}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(fitted, resid, alpha=0.5)\n",
        "    plt.axhline(0, color='k', linestyle='--')\n",
        "    plt.xlabel('Fitted')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title(f\"Residuals vs Fitted — {feat}\")\n",
        "    plt.savefig(os.path.join(OUT_DIR,f\"resid_fitted_{feat}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    assumption_flag = {\n",
        "        'normal_resid': bool(sh_p>alpha) if not np.isnan(sh_p) else False,\n",
        "        'homoscedastic': bool(lev_p>alpha) if not np.isnan(lev_p) else False\n",
        "    }\n",
        "\n",
        "    if assumption_flag['normal_resid'] and assumption_flag['homoscedastic'] and pval < alpha:\n",
        "        try:\n",
        "            tukey = pairwise_tukeyhsd(endog=sub[feat], groups=sub['NObeyesdad_new'], alpha=alpha)\n",
        "            tuk_df = pd.DataFrame(data=tukey._results_table.data[1:], columns=tukey._results_table.data[0])\n",
        "            tuk_df['feature'] = feat\n",
        "            tuk_df.to_csv(os.path.join(OUT_DIR,f\"tukey_{feat}.csv\"), index=False)\n",
        "            # collect summary\n",
        "            for _,r in tuk_df.iterrows():\n",
        "                posthoc_rows.append({'feature':feat,'group1':r['group1'],'group2':r['group2'],'meandiff':r['meandiff'],'p_adj':float(r['p-adj']),'reject':r['reject']})\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    if not assumption_flag['homoscedastic'] and pval < alpha:\n",
        "        import itertools\n",
        "        pairs = []\n",
        "        levels_list = sub['NObeyesdad_new'].unique().tolist()\n",
        "        for a,b in itertools.combinations(levels_list,2):\n",
        "            s1 = sub.loc[sub['NObeyesdad_new']==a, feat].dropna()\n",
        "            s2 = sub.loc[sub['NObeyesdad_new']==b, feat].dropna()\n",
        "            if len(s1)<2 or len(s2)<2:\n",
        "                continue\n",
        "\n",
        "            t, p = stats.ttest_ind(s1, s2, equal_var=False)\n",
        "            pairs.append({'feature':feat,'group1':a,'group2':b,'t':float(t),'p_uncorrected':float(p)})\n",
        "        if len(pairs)>0:\n",
        "            pvals = [pp['p_uncorrected'] for pp in pairs]\n",
        "            _, pvals_adj, _, _ = multipletests(pvals, method='fdr_bh', alpha=alpha)\n",
        "            for i,pp in enumerate(pairs):\n",
        "                pp['p_adj'] = float(pvals_adj[i]); pp['reject'] = pvals_adj[i] < alpha\n",
        "                welch_pairwise_rows.append(pp)\n",
        "\n",
        "pd.DataFrame(anova_summary_rows).to_csv(os.path.join(OUT_DIR,\"anova_summary.csv\"), index=False)\n",
        "pd.DataFrame(assumption_rows).to_csv(os.path.join(OUT_DIR,\"assumption_checks.csv\"), index=False)\n",
        "pd.DataFrame(posthoc_rows).to_csv(os.path.join(OUT_DIR,\"tukey_posthoc.csv\"), index=False)\n",
        "pd.DataFrame(welch_pairwise_rows).to_csv(os.path.join(OUT_DIR,\"welch_pairwise_fdr.csv\"), index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}